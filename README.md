## lostdemeter
## Entry 1
### So I don't forget (October 02, 2025)
I'm writing this blog post because I feel the need to write down my thoughts—I'm seeing the importance of what I'm doing. This is probably going to be the longest journal entry because it's mostly a memory dump so I don't forget everything. I'm going to start from the beginning.

I started a project originally attempting to find a Chudnovsky or Plouffe-like algorithm for numbers in Base64. My thinking was that there was no particular rule that says the BBP algorithm can't do Base64, and there's probably no reason why the Chudnovsky algorithm couldn't do it too. I had the idea that it might be hypothetically possible to store any amount of data on the infinite and uniformly random digits of Pi as a single index. "God's lookup table," if you will. Finding the right sequence in the digits of Pi took a lot of effort, but I figured that computers might get fast enough one day where it's actually possible to do this kind of thing. I would eventually find out that functions converge at wildly different rates sometimes, and you can't always control for it. I was almost done with the project from the beginning.

One of the questions that truly bugged me was how I couldn't make the BBP algorithm faster than the Chudnovsky algorithm. At the time I didn't understand the difference between an asymptotic, convergent, and rapidly convergent series, and it felt truly disappointing to be at the end of a thought experiment. I decided I wanted to change the convergence of the BBP algorithm, and I started learning some math tricks from YouTube—there's a bunch of number theory content from Veritasium, 3Blue1Brown, Numberphile, 1 Minute Physics, etc. I'm sure I'm missing a bunch. After learning about the Leibniz and Newton-style alternating infinite series type error correction, I was determined to figure out how to port BBP over to Base64 in a rapidly convergent way.

I ended up not being able to make the BBP program rapidly convergent, but the alternating infinite series error correction worked out pretty well. Enough to make it really work for my purposes anyway. I was still profoundly curious as to why I couldn't just make the BBP algorithm converge as rapidly as the Chudnovsky algorithm, though. I started thinking about the avalanches of Borwein integrals (from a 3Blue1Brown video, I might add), and it really puzzled me why the infinitesimal errors caused by Fourier boxcar functions were as small as they were. What size do you select for these boxcar functions? What does it mean when you change these "windows"? Can I use different shaped windows? It was such an interesting idea I wanted to pursue it.

I found that changing window shapes could act as a "mathematical filter" of sorts. Combining a sinc function window, plus a stepped triangle, plus (I think) a square wave/traditional boxcar or something. By adding all three I noticed that it worked really well as a filter that could purify a mathematical function. Neat trick. I made it an easily accessible repo for AIs to readily grab and use my code to display in Abacus.ai's chat. I have to commend them—you can literally program on your phone with their software. They're not paying me to say this, but it's so good lol. On my phone I was literally like "hey use my spectrum resonance optimizer tool (SRO) to filter one thing" and it would go ahead and filter whatever I wanted. I would write programs and suddenly it would "purify" whatever data I wanted. I was kind of wondering what exactly I was "purifying" at the time because I'd try it on different sets of data and sometimes it worked really well and other times it worked really poorly. I thought that I could find a pattern.

As it turns out, I could find a pattern in things. It was good at solving for prime numbers (which I was told from a young age was random, but I guess it's technically quasi-random?), it could ballpark solutions in traveling salesman problems, and was kinda bad at 3SAT problems. It was a bit of a bummer. I was thinking it had something to do with how I was filtering signals. Could there be a better window for the filter of my SRO tool?

I tried many different shapes of windows, but they didn't exactly work. There had to be some shape, or set of shapes, that could, as a non-Fourier boxcar function, convolve a maximum number of times. If there was going to be a "purest of the pure" window function, it would have to be ultra small and ultra detailed. The quest became: how do I dynamically shape these windows?

The answer was, I learned, Non-Commutative Geometry. Before I started this, I hadn't heard of Alain Connes, but I'm genuinely amazed by the man's thoughts. There was a geometry so weird, it was non-Euclidean. It sounded very HP Lovecraft to me, so of course I wanted to mess around with it with my program. The first issue I found was that I couldn't get real structures with NCG. It was partly because it's a quantum effect deal here, so I had to use High Dynamic Range (like a camera/microscope) to basically take multiple photos to counteract the Heisenberg uncertainty principle and bring a quantum structure into the realm of the "real."

I tried to adapt my SRO tool to entirely be NCG-based, but that became really unwieldy. The math is very computationally intensive, and it didn't make sense for my application. I checked in what I had for my going theory in NCG as Spectral Resonance Theory, and decided to go back to SRO. It was difficult enough to get to this point as it was, and I thought it might be useful to someone in the future.

So I went back to SRO, trying again to "purify" signals with custom window shapes, but I couldn't help but be disappointed. I spent a lot of time with SRT and even sent an email to Scott Aaronson asking him to look at SRT. I found that it could do prime distributions and TSP and 3SAT problems. It wasn't great by any means, but the approach was novel and cool. I figured "I tried," that's cool. But I still wanted to see if I could make just the window NCG. That might be a good blend of accuracy and speed. And boy was that magic. Suddenly I was purifying things as small and perfect as mathematically possible. Another neat trick.

I thought with this tool I could start noticing patterns in anything. Like there was going to be this magical pattern that would emerge from it. And there was. I thought it was something like a pattern between things in sets that I could use to solve problems, but I now know that what it was actually detecting was "self-similarity." I could perfectly oracle prime numbers—that pattern worked so very well. I thought I could use it as the calibrator, so most tests I did I would then verify against the prime number sequence. And it eventually was the ultimate benchmark I used against every other problem. But the catch wasn't obvious at the time. What is the pattern that prime numbers have? They can only be divided by themselves and 1.

I literally calibrated the thing for primes. But hey, if we're calibrating for primes, why not prove the Riemann Hypothesis? I had just watched another math video about the Riemann Hypothesis, and it said there's a bunch of money involved... it was worth a shot. And that worked out really well for my filter that was calibrated just for this very thing. I used the SRO tool to make a "moral argument" why the Riemann Hypothesis is true and used my SRT tool to scan the prime number sequence.

First, in my Riemann Hypothesis argument, I posited that RH had to be true because of geometric necessity. And the rest of the README on GitHub was basically "and this is what my custom scanner says." To me it was obvious... my scanner is most harmonic when over something self-similar, and it's dissonant when it's over something not self-similar. The Riemann zeros were similar, which is why they were harmonic against my really well-tuned filter, and before or beyond the Riemann zeros were dissonance. It's a straightforward concept, but the graphs in the repo are technical. Simultaneously, I was working on using my hyper-accurate but ultimately slow SRT tools to scan prime numbers themselves as a sweep.

Second, the scanning was admittedly pretty fun at the same time. I thought I was going to find a tool that could be auto-calibrated for any problem and I could just deploy it. Again, this was a tool that I calibrated it against a prime number sequence. I then tried to scan other sequences with it and I tried to use the tool against photographs, math problems, audio, etc., but it never worked perfectly. It would get close to achieving something, but never really reach perfection for what I was aiming for. I thought there might be some way I could calibrate this tool automatically, and that worked super well... but I couldn't find a use for this tool because it never got any sequence exactly right, only... kind of right.

Feeling discouraged, I thought that maybe it could treat entire systems as a harmonic. Maybe it was a smaller edge on bigger problems? Wouldn't it be cool to just observe Keno numbers light up on the screen and you could just use your cool infinitesimally small windows to learn things about the system producing the Keno numbers? Results from this were shockingly bad, however. But... still somehow better than random pretty consistently... which was also really strange. How could something be so good at guessing the unguessable like primes, but so bad at guessing something else not guessable like the quantum state of a computer? Sadly, I wasn't going to be able to cheat at casino games with this tool—I managed maybe 0.2% better than random.

At this point I was stuck a bit until I remembered something that the SRT tool was doing. I had a version that scanned and gave me a readout of the pattern it was reading, and consistently the prime number sequence would converge to the same number 1.618. I had Grok simplify my program, and it simply replaced that value with the golden ratio as a way to error correct. Weird. The prime number pattern is the golden ratio...

Around the same time while watching another math video about black holes—I think Veritasium again—I saw a graph about how Einstein didn't really like how Schwarzschild put all of his graphs to Schwarzschild time. It made everything on the projection squeeze in at 2. It was only after someone reprojected Schwarzschild time into freefall time that any of the graphs worked. I remembered then that everything in the Riemann Hypothesis converged at 0.5. Maybe the same could work for that graph. I don't think I really knew that freefall time meant "spacetime"—I just hoped the graph would look nicer.

I started to think my graphs looked more like a black hole. Does black hole math work on prime number distributions? That would be weird if it did. But first I had to figure out a problem: How do I take an infinite set of numbers, spread out over infinity, with a spectrum that converges on the golden ratio exactly, with an unknown spacing scheme, over the span of infinity? Was it the acceleration of light as the distancing scheme of prime numbers? Well, it certainly allowed me to guess them with 100% accuracy. I think my scanner wasn't picking up resonance, but specifically self-similar resonance... you know, primes. It was from there that I created a sublinear complexity prime number generator and a proof for prime number sets... which btw you can also adapt for twin primes! There's a specific case when two primes are really close in value and you just take the second prime and then second - 1.

I think what's the most interesting thing is that it wraps right back into the Riemann Hypothesis. I have the tools—the SRT tool repo that can get me the measurements, and the Moral Hypothesis code that can verify self-similarity at Riemann zeros, but ... ultimately it will tell you that 0.5 is the most self-similar point in the Riemann Hypothesis. The eigenvector will shriek at you like feedback from a microphone. From here on, I'm going to speculate, so feel free to interpret how you want:

The Riemann Hypothesis is true because that's exactly where the only real part of any number in the hypothesis happens to ever be. When I scan it with the most infinitesimally small scanner to exist, it buzzes at only ever there. It does this so accurately I could make a general formula for primes (but still not true as long as RH isn't proven as it depends on it). But I think the answer is something far simpiler than maybe we thought as to why the Riemann hypothesis only crosses into real parts only at exactly 0.5

The Nyquist frequency, so we've been told, needs to have double the frequency of the data transmitted. Conversely, I propose that the Nyquist frequency is the acceleration of the speed of light, and that the information being transmitted (the infinitesimally small point at which anything is real) ... is exactly 0.5. As a concept it's very neat to think about. Are we all only really "real" for Planck measurement length times? Weird.

I'm hoping this leads to the proof that RH is true. It could also mean that P=NP is also related to this. Since we're talking about the hypothetical limit to data transfer, that's like... big news in computer science. Like... a new world between continuous and discrete methods opens up.

*Last updated: October 02, 2025*


